{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_FOLDER = \"C:/Users/Killian/Desktop/FYP-Multistage-Throughput-Predictor/Datasets/Final_Outputs/\"\n",
    "IMAGES_OUTPUT_FOLDER = \"C:/Users/Killian/Desktop/FYP-Multistage-Throughput-Predictor/Own Papers/Undergraduate Paper/Images/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(model_name=\"univariate_baseline\", just_preds=False, mb=True):\n",
    "    if just_preds:\n",
    "        predicted = np.load(DATASETS_FOLDER+model_name+\"_predicted_y.npy\")\n",
    "        if mb:\n",
    "            return predicted/1024\n",
    "        return predicted\n",
    "    true = np.squeeze(np.load(DATASETS_FOLDER+model_name+\"_true_y.npy\"))\n",
    "    predicted = np.load(DATASETS_FOLDER+model_name+\"_predicted_y.npy\")\n",
    "    if mb:\n",
    "        return true/1024, predicted/1024\n",
    "    return true, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance(data):\n",
    "    return np.var(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(data):\n",
    "    return np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_residuals(true, pred):\n",
    "        residuals = true-pred\n",
    "        return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def time_step_boxplots(residuals=[], model_names=[], title=\"\", xlim=(), savefig_path=\"\", xlab=\"Residuals (Mbps)\",showfliers = True):\n",
    "    df = pd.DataFrame()\n",
    "    for model, residuals in zip(model_names, residuals):\n",
    "        for i in range(residuals.shape[1]):\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df[xlab] = residuals[:,i]\n",
    "            temp_df[\"Horizon (seconds)\"] = i+1\n",
    "            temp_df[\"Model\"] = model\n",
    "            df = pd.concat([df, temp_df])\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    if showfliers:\n",
    "        box = sns.boxplot(x=xlab, y=\"Horizon (seconds)\",\n",
    "                    hue=\"Model\",\n",
    "                    data=df, orient=\"h\", whis=(5,95), saturation=1, flierprops={'marker': 'o', 'markersize': 5, 'markerfacecolor': 'pink'})\n",
    "    else:\n",
    "        box = sns.boxplot(x=xlab, y=\"Horizon (seconds)\",\n",
    "            hue=\"Model\",\n",
    "            data=df, orient=\"h\", whis=(5,95), saturation=1, showfliers=False)\n",
    "    if xlim:\n",
    "        box.set_xlim(xlim)\n",
    "    box.set_title(title)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    # box.set_ylabel(\"Horizon (seconds)\")\n",
    "    if savefig_path:\n",
    "        plt.savefig(savefig_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show() # Display the plot on the screen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(true, predicted, epsilon=50/1024):\n",
    "    denominator = np.squeeze(true) + epsilon\n",
    "    try:\n",
    "        mape = np.mean(np.abs((np.squeeze(true) - predicted)/denominator))*100\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        mape = \"n/a\"\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ape(true, predicted, epsilon=50/1024):\n",
    "    denominator = np.squeeze(true) + epsilon\n",
    "    try:\n",
    "        ape = np.abs((np.squeeze(true) - predicted)/denominator)*100\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ape = \"n/a\"\n",
    "    return ape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(true, predicted):\n",
    "    mse = np.mean(np.power(np.squeeze(true)-predicted, 2))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(true, predicted):\n",
    "    mae = np.mean(np.abs(np.squeeze(true)-predicted))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(table):\n",
    "    latex_string = \"\\\\begin{table}[!htb]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|c|c|c|c|}\\n\\\\hline\\n{Model} & {Mean Resids (Mbps)} & {Resids std (Mbps)} & {MAPE} & {MSE (Mbps)} & {MAE (Mbps)}\\\\\\\\\\n\\\\hline\\n\"\n",
    "    for row in table:\n",
    "        for i in range(len(row)):\n",
    "            if i == len(row) - 1:\n",
    "                latex_string += str(row[i]) + \"\\\\\\\\\\n\\\\hline\\n\"\n",
    "            else:\n",
    "                latex_string += str(row[i]) + \" & \"\n",
    "    latex_string_end = \"\\\\end{tabular}\\n\\\\label{tab:train_test_dist}\\n\\\\end{table}\"\n",
    "    latex_string = latex_string + latex_string_end\n",
    "    print(latex_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_analysis(prefix=\"univariate\", title_prefix=\"Univariate\"):\n",
    "    for dataset in [\"low\", \"medium\", \"high\", 1]:\n",
    "        if dataset == 1:\n",
    "            base_true, base_pred = import_data(model_name=\"{}_baseline\".format(prefix))\n",
    "            multiOne_true, multiOne_pred = import_data(model_name=\"{}_multiOne\".format(prefix))\n",
    "            multiAll_true, multiAll_pred = import_data(model_name=\"{}_multiAll\".format(prefix))\n",
    "\n",
    "            # Get Residuals\n",
    "            base_resids = get_ts_residuals(base_true, base_pred)\n",
    "            multiOne_resids = get_ts_residuals(multiOne_true, multiOne_pred)\n",
    "            multiAll_resids = get_ts_residuals(multiAll_true, multiAll_pred)\n",
    "\n",
    "            # Get Absolute Percent Error\n",
    "            base_ape = get_ape(base_true, base_pred)\n",
    "            multiOne_ape = get_ape(multiOne_true, multiOne_pred)\n",
    "            multiAll_ape = get_ape(multiAll_true, multiAll_pred)\n",
    "            \n",
    "            # Boxplot of residuals\n",
    "            time_step_boxplots(residuals=[base_resids, multiOne_resids, multiAll_resids],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Model Performance on All Test Sequences\",\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"{} On All Test Data\".format(title_prefix))\n",
    "            \n",
    "            # Boxplot of Absolute Percent Error\n",
    "            time_step_boxplots(residuals=[base_ape, multiOne_ape, multiAll_ape],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Absolute Percent Error on All Test Sequences\",\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"APE of {} On All Test Data\".format(title_prefix), xlab=\"Absolute Percent Error\")\n",
    "            \n",
    "            # Boxplot of residuals No outliers\n",
    "            time_step_boxplots(residuals=[base_resids, multiOne_resids, multiAll_resids],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Model Performance on All Test Sequences\",\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"{} On All Test Data no Outliers\".format(title_prefix), showfliers = False)\n",
    "            \n",
    "            # Boxplot of Absolute Percent Error no Outliers\n",
    "            time_step_boxplots(residuals=[base_ape, multiOne_ape, multiAll_ape],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Absolute Percent Error on All Test Sequences\",\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"APE of {} On All Test Data no Outliers\".format(title_prefix), xlab=\"Absolute Percent Error\", showfliers = False)\n",
    "\n",
    "\n",
    "            # Standard Dev\n",
    "            base_std = get_std(base_resids)\n",
    "            multiOne_std = get_std(multiOne_resids)\n",
    "            multiAll_std = get_std(multiAll_resids)\n",
    "\n",
    "            # MAPE\n",
    "            base_mape = get_mape(base_true, base_pred)\n",
    "            multiOne_mape = get_mape(multiOne_true, multiOne_pred)\n",
    "            multiAll_mape = get_mape(multiAll_true, multiAll_pred)\n",
    "\n",
    "            # MSE\n",
    "            base_mse = get_mse(base_true, base_pred)\n",
    "            multiOne_mse = get_mse(multiOne_true, multiOne_pred)\n",
    "            multiAll_mse = get_mse(multiAll_true, multiAll_pred)\n",
    "\n",
    "            # MAE\n",
    "            base_mae = get_mae(base_true, base_pred)\n",
    "            multiOne_mae = get_mae(multiOne_true, multiOne_pred)\n",
    "            multiAll_mae = get_mae(multiAll_true, multiAll_pred)\n",
    "\n",
    "            # Table of metrics\n",
    "            table = PrettyTable([\"Model\", \"Mean Resids (Mbps)\", \" Resids std (Mbps)\", \"MAPE\", \"MSE\", \"MAE\"])\n",
    "            table.add_row([\"Baseline\",np.mean(base_resids), base_std, base_mape, base_mse, base_mae])\n",
    "            table.add_row([\"Multistage One\",np.mean(multiOne_resids), multiOne_std, multiOne_mape, multiOne_mse, multiOne_mae])\n",
    "            table.add_row([\"Multistage All\",np.mean(multiAll_resids), multiAll_std, multiAll_mape, multiAll_mse, multiAll_mae])\n",
    "            print(\"Residuals of All Sequences Test Set\")\n",
    "            #print(table)\n",
    "            raw_table = []\n",
    "            raw_table.append([\"Baseline\",round(np.mean(base_resids),3), round(base_std,3), round(base_mape,3), round(base_mse,3), round(base_mae,3)])\n",
    "            raw_table.append([\"Multistage One\",round(np.mean(multiOne_resids),3), round(multiOne_std,3), round(multiOne_mape,3), round(multiOne_mse,3), round(multiOne_mae,3)])\n",
    "            raw_table.append([\"Multistage All\",round(np.mean(multiAll_resids),3), round(multiAll_std,3), round(multiAll_mape,3), round(multiAll_mse,3), round(multiAll_mae,3)])\n",
    "            create_latex_table(raw_table)\n",
    "        else:\n",
    "            base_true, base_pred = import_data(model_name=\"{}_baseline_{}\".format(prefix, dataset))\n",
    "            single_true, single_pred = import_data(model_name=\"{}_multiOne_{}\".format(prefix, dataset))\n",
    "\n",
    "            # Get Residuals\n",
    "            base_resids = get_ts_residuals(base_true, base_pred)\n",
    "            single_resids = get_ts_residuals(single_true, single_pred)\n",
    "\n",
    "            # Get APE\n",
    "            base_ape = get_ape(base_true, base_pred)\n",
    "            single_ape = get_ape(single_true, single_pred)\n",
    "            \n",
    "            # Boxplot of residuals\n",
    "            time_step_boxplots(residuals=[base_resids, single_resids],\n",
    "                                model_names=[\"Baseline\", \"{} Only\".format(dataset)],\n",
    "                                  title=\"Model Performance on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"{} On {} Test Set\".format(title_prefix, dataset))\n",
    "\n",
    "            # Boxplot of APE\n",
    "            time_step_boxplots(residuals=[base_ape, single_ape],\n",
    "                                model_names=[\"Baseline\", \"{} Only\".format(dataset)],\n",
    "                                  title=\"Absolute Percent Error on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"APE of {} On {} Test Set\".format(title_prefix, dataset), xlab=\"Absolute Percent Error\")\n",
    "            \n",
    "            # Boxplot of residuals no Outliers\n",
    "            time_step_boxplots(residuals=[base_resids, single_resids],\n",
    "                                model_names=[\"Baseline\", \"{} Only\".format(dataset)],\n",
    "                                  title=\"Model Performance on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"{} On {} Test Set no Outliers\".format(title_prefix, dataset), showfliers = False)\n",
    "\n",
    "            # Boxplot of APE no Outliers\n",
    "            time_step_boxplots(residuals=[base_ape, single_ape],\n",
    "                                model_names=[\"Baseline\", \"{} Only\".format(dataset)],\n",
    "                                  title=\"Absolute Percent Error on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"APE of {} On {} Test Set no Outliers\".format(title_prefix, dataset), xlab=\"Absolute Percent Error\", showfliers = False)\n",
    "\n",
    "          ###############\n",
    "            multiOne_pred = import_data(model_name=\"{}_multiOne_ms_{}\".format(prefix, dataset), just_preds=True)\n",
    "            multiAll_pred = import_data(model_name=\"{}_multiAll_ms_{}\".format(prefix, dataset), just_preds=True)\n",
    "\n",
    "            #resids\n",
    "            multiOne_resids = get_ts_residuals(single_true, multiOne_pred)\n",
    "            multiAll_resids = get_ts_residuals(single_true, multiAll_pred)\n",
    "\n",
    "            multiOne_ape = get_ape(single_true, multiOne_pred)\n",
    "            multiAll_ape = get_ape(single_true, multiAll_pred)\n",
    "\n",
    "            # Boxplot of residuals\n",
    "            time_step_boxplots(residuals=[base_resids, multiOne_resids, multiAll_resids],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Model Performance on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"MS {} On {} Test Set\".format(title_prefix, dataset))\n",
    "\n",
    "            # Boxplot of APE\n",
    "            time_step_boxplots(residuals=[base_ape, multiOne_ape, multiAll_ape],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Absolute Percent Error on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"MS APE of {} On {} Test Set\".format(title_prefix, dataset), xlab=\"Absolute Percent Error\")\n",
    "            \n",
    "            # Boxplot of residuals no Outliers\n",
    "            time_step_boxplots(residuals=[base_resids, multiOne_resids, multiAll_resids],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Model Performance on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"MS {} On {} Test Set no Outliers\".format(title_prefix, dataset), showfliers = False)\n",
    "\n",
    "            # Boxplot of APE no Outliers\n",
    "            time_step_boxplots(residuals=[base_ape, multiOne_ape, multiAll_ape],\n",
    "                                model_names=[\"Baseline\", \"Multistage One\", \"Multistage All\"],\n",
    "                                  title=\"Absolute Percent Error on {} Test Sequences\".format(dataset.capitalize()),\n",
    "                                  savefig_path=IMAGES_OUTPUT_FOLDER+\"MS APE of {} On {} Test Set no Outliers\".format(title_prefix, dataset), xlab=\"Absolute Percent Error\", showfliers = False)\n",
    "                        # Standard Dev\n",
    "            base_std = get_std(base_resids)\n",
    "            multiOne_std = get_std(multiOne_resids)\n",
    "            multiAll_std = get_std(multiAll_resids)\n",
    "\n",
    "            # MAPE\n",
    "            base_mape = get_mape(base_true, base_pred)\n",
    "            multiOne_mape = get_mape(single_true, multiOne_pred)\n",
    "            multiAll_mape = get_mape(single_true, multiAll_pred)\n",
    "\n",
    "            # MSE\n",
    "            base_mse = get_mse(base_true, base_pred)\n",
    "            multiOne_mse = get_mse(single_true, multiOne_pred)\n",
    "            multiAll_mse = get_mse(single_true, multiAll_pred)\n",
    "\n",
    "            # MAE\n",
    "            base_mae = get_mae(base_true, base_pred)\n",
    "            multiOne_mae = get_mae(single_true, multiOne_pred)\n",
    "            multiAll_mae = get_mae(single_true, multiAll_pred)\n",
    "            \n",
    "            \n",
    "            raw_table = []\n",
    "            raw_table.append([\"Baseline\",round(np.mean(base_resids),3), round(base_std,3), round(base_mape,3), round(base_mse,3), round(base_mae,3)])\n",
    "            raw_table.append([\"Multistage One\",round(np.mean(multiOne_resids),3), round(multiOne_std,3), round(multiOne_mape,3), round(multiOne_mse,3), round(multiOne_mae,3)])\n",
    "            raw_table.append([\"Multistage All\",round(np.mean(multiAll_resids),3), round(multiAll_std,3), round(multiAll_mape,3), round(multiAll_mse,3), round(multiAll_mae,3)])\n",
    "            print(\"\\n================\\nMULTISTAGE\\n===============\\n\")\n",
    "            create_latex_table(raw_table)\n",
    "            print(\"\\n================\\n\")\n",
    "          #####################\n",
    "\n",
    "            # Standard Dev\n",
    "            base_std = get_std(base_resids)\n",
    "            single_std = get_std(single_resids)\n",
    "\n",
    "            # MAPE\n",
    "            base_mape = get_mape(base_true, base_pred)\n",
    "            single_mape = get_mape(single_true, single_pred)\n",
    "\n",
    "            # MSE\n",
    "            base_mse = get_mse(base_true, base_pred)\n",
    "            single_mse = get_mse(single_true, single_pred)\n",
    "\n",
    "            # MAE\n",
    "            base_mae = get_mae(base_true, base_pred)\n",
    "            single_mae = get_mae(single_true, single_pred)\n",
    "\n",
    "            # Table of metrics\n",
    "            table = PrettyTable([\"Model\", \"Mean Resids (Mbps)\", \" Resids std (Mbps)\", \"MAPE\", \"MSE\", \"MAE\"])\n",
    "            table.add_row([\"Baseline\",np.mean(base_resids), base_std, base_mape, base_mse, base_mae])\n",
    "            table.add_row([\"{} Only\".format(dataset),np.mean(single_resids), single_std, single_mape, single_mse, single_mae])\n",
    "            print(\"Residuals of {} Sequences Test Set\".format(dataset))\n",
    "            #print(table)\n",
    "            \n",
    "            # Latex\n",
    "            raw_table = []\n",
    "            raw_table.append([\"Baseline\",round(np.mean(base_resids),3), round(base_std,3), round(base_mape,3), round(base_mse,3), round(base_mae,3)])\n",
    "            raw_table.append([\"{} Only\".format(dataset),round(np.mean(single_resids),3), round(single_std,3), round(single_mape,3), round(single_mse,3), round(single_mae,3)])\n",
    "            create_latex_table(raw_table)\n",
    "\n",
    "    true, pred = import_data(model_name=\"{}_multiOne_classifier\".format(prefix))\n",
    "    true = np.argmax(true, axis=-1)\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    conf_matrix = confusion_matrix(true, pred)\n",
    "    # create a dictionary to map class names\n",
    "    class_names = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "\n",
    "    # map the class names\n",
    "    true_names = [class_names[x] for x in true]\n",
    "    pred_names = [class_names[x] for x in pred]\n",
    "    report = classification_report(true_names, pred_names, labels=[\"low\", \"medium\", \"high\"])\n",
    "    print(\"Classification Report:\\n\",report)\n",
    "    display_conf = ConfusionMatrixDisplay(conf_matrix, display_labels=[\"Low\", \"Medium\", \"High\"])\n",
    "    display_conf.plot(cmap=\"Oranges\")\n",
    "    plt.savefig(IMAGES_OUTPUT_FOLDER+\"{} Confusion Matrix\".format(title_prefix), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_out_analysis(\"constraint_1_5\", \"Size Constraint of 1 and a half MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_out_analysis(\"constraint_3\", \"Constraint 3Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_out_analysis(\"constraint_1_5\", \"Size Constaint 1_5Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_out_analysis(\"H15H5\", \"H15H5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Killian/Desktop/FYP-Multistage-Throughput-Predictor/Datasets/Final_Outputs/univariate_classifier_true_y.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m true, pred \u001b[39m=\u001b[39m import_data(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39munivariate_classifier\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(true, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(pred, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mimport_data\u001b[1;34m(model_name, just_preds, mb)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[39mreturn\u001b[39;00m predicted\u001b[39m/\u001b[39m\u001b[39m1024\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted\n\u001b[1;32m----> 7\u001b[0m true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(np\u001b[39m.\u001b[39;49mload(DATASETS_FOLDER\u001b[39m+\u001b[39;49mmodel_name\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_true_y.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m predicted \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(DATASETS_FOLDER\u001b[39m+\u001b[39mmodel_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_predicted_y.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m mb:\n",
      "File \u001b[1;32mc:\\Users\\Killian\\miniconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Killian/Desktop/FYP-Multistage-Throughput-Predictor/Datasets/Final_Outputs/univariate_classifier_true_y.npy'"
     ]
    }
   ],
   "source": [
    "true, pred = import_data(model_name=\"univariate_classifier\")\n",
    "true = np.argmax(true, axis=-1)\n",
    "pred = np.argmax(pred, axis=-1)\n",
    "conf_matrix = confusion_matrix(true, pred)\n",
    "# create a dictionary to map class names\n",
    "class_names = {0: \"low\", 1: \"medium\", 2: \"high\"}\n",
    "\n",
    "# map the class names\n",
    "true_names = [class_names[x] for x in true]\n",
    "pred_names = [class_names[x] for x in pred]\n",
    "report = classification_report(true_names, pred_names, labels=[\"low\", \"medium\", \"high\"])\n",
    "print(\"Classification Report:\\n\",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a0f0acd0089a531616946ee2a9708cd6d3e1ac7fcc3f79f44e37592fb92385f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
