\chapter{Introduction}
Total mobile network traffic has doubled over the last two years and is expected to increase a by a factor of 4 by the year 2028. This is due to growth in the number of mobile devices as well as the use of 5G mobile networks \cite{Eri22}. Cellular networks are highly dynamic systems. The system conditions are highly variable and depend on various constantly changing factors such as the level of interference, location, line of site etc. Throughput is heavily dependent on the condition current condition of the network and as such can vary dramatically of the course of a few seconds. This presents a challenge for both the network itself, and the mobile devices that leverage the network. The network attempts to balance its available resources between all of its connected users and applications running on the mobile devices require knowledge of the available throughput in order to function correctly. 

A notable use case of throughput prediction on cellular networks relates to video streaming. The vast majority of mobile network traffic is video streaming. \cite{Eri22}. Video streaming applications require high network throughput and good consistency in order to deliver the video quality we expect in today's world. Video streaming requires that chunks of the video be downloaded in a timely manner before they are used. The application will dynamically adjust the size of the chunk of the video it attempts to download depending on throughput predictions. For example if the throughput prediction algorithm in use predicts a low download bitrate over the next few seconds the application will schedule the download of a lower quality video chunk as lower quality means less overall data. Failure to adapt to the current network environment may result in buffering or noticeable changes in quality throughout a video for the user. Accurate throughput prediction plays a key role in maintaining the quality of user experience in these applications \cite{raca2019improving}. 

Adoption of currently niche technologies such as AR (augmented reality) and VR (virtual reality) is projected to increase in the near future \cite{Sta22}. These technologies will make heavy use of cellular networks for device mobility and ergonomics. VR and AR devices require low latency and high throughput connections to function optimally. Right now this is achieved by tethering such devices to a network via a physical wire. This limits the potential use cases and adoption of such technologies. 5G can provide the latency and throughput requirements for optimal operation of these technologies allowing them to be used in more mobile devices. As such they stand to benefit from accurate throughput prediction.

Multistage models make use of 2 or more models to solve a given problem. This can be done when the problem can be broken up into smaller pieces which can be solved separately. The models may be linked together sequentially or in parallel depending on how the problem was decomposed. The intuition behind using a multistage approach as opposed to tradition single model approaches is that throughput in a cellular channel is inherently inbalanced. As seen in this paper, typical download throughput levels fall in the ranges above 5Mbps. User experience is disproportionally influenced by low throughput situations such as <5Mbps or <1Mbps as opposed to differences experienced outside these bounds \cite{raca2019improving}. Models trained on smaller sub-ranges of the data should make better predictions in these ranges compared to a single model trained on data related to the entire throughput range.

\section{Background on Cellular Networks}
\textbf{INCLUDE ILLUSTRATIONS}
A basic understanding of the cellular network architecture will help with understanding the dataset used in this project. Cellular networks, be that 4G, 5G or any previous generation use electromagnetic waves to carry data. The access points for cellular networks are referred to as base stations or sometimes as cell towers. Base stations are what mobile devices such as your phone or laptop will connect to for its internet connection. As the main point of contact for mobile devices, we will focus our understanding of cellular networks solely on the interaction between mobile devices and base stations.

The provider of a cellular network is allocated a frequency band which it can use to provide internet connection. Typically mobile providers would have multiple bands to leverage for different use cases. Bands spanning higher frequency ranges are capable of carrying more data (higher bitrate), however they suffer from having short range and less of an ability to penetrate  common obstructions such as walls. Lower frequency bands conversely, have longer range and a better ability to penetrate obstacles but a lower bitrate. As such lower frequency bands tend to be used to serve rural areas, where the longer range and better penetration are best utilised and higher frequency waves are used in densely populated areas such as cities or towns.

When a mobile device connects to a cellular network it is allocated a sub-section of the frequency band of the provider. The width of the band allocated to the mobile device depends on the bandwidth available as well as the bitrate required for the connection. For 4G LTE, the smallest sub-band a user can be allocated 180kHz in width. From this sub-band 12 orthogonal frequencies are chosen. Orthogonal in this case means that the electromagnetic waves do not interfere with one another. Data can be encoded in each of these waves individually. As such the bitrate a user has access to is a product of the number of orthogonal waves they are allocated (bandwidth) as well as the frequency of said waves.

With the deployment of 5G comes the use of higher frequency bands (24GHz +). Because of the shorter range and low penetration at these frequencies more base stations will be required to cover the same area compared to before. As such, it is likely such bands will be deployed in densely populated areas such as cities or towns where they will get the most use. Mobile devices will have to jump between base stations more often in order to maintain a connection. Obstructions are also abundant in such areas which may lead to intermittent connection disruption, from simple buildings to more dynamic obstructions such as cars or even other people. There is also more electromagnetic interference due to the density of devices that leverage wireless communication technologies. Accurate throughput prediction could play a vital role in the quality of service in such environments.


\section{Throughput Prediction for Cellular Networks}

Statistical estimation of cellular channel conditions is known to be highly difficult due to the variability of such environments \cite{10.1145/2785956.2787498}. Throughput prediction involves the use of various metrics available to a mobile device to estimate future throughput scenarios. Data used can be both present and historical and as such this problem is essentially a time series forecasting problem. Data available to a mobile device includes metrics relating to the physical layer connection between it an the base station, metrics from neighbouring base stations as well as location data. As it stands, data relating to the overall state of the network is not readily available to a mobile device further adding to the difficulty in estimation. As the state of the channel depends on a large number of factors, machine learning (ML) and deep learning (DL) techniques are likely well suited for tackling this issue. ML \& DL have proven themselves in other networking related problems in the past \cite{8666641}. LSTM (long short-term memory) networks are recognised as effective models for time series forecasting \cite{8614252}. This paper explores the use of Lstm models in a multistage architecture to preform throughput prediction.
